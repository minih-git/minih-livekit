"""
æœ¬åœ° ASR æ¨¡å— - ä½¿ç”¨ Sherpa-onnx è¿›è¡Œå®æ—¶è¯­éŸ³è¯†åˆ« (ä¿®å¤ç‰ˆ)
æ”¹è¿›ç‚¹ï¼š
    1. VAD é¢„è¯»ç¼“å†² (Lookback Buffer) - ä¿®å¤å¼€å¤´ä¸¢å­—é—®é¢˜
    2. å£°é“æå–ä¼˜åŒ– - ä¿®å¤åŒå£°é“æ··åˆå¯¼è‡´çš„éŸ³é‡è¡°å‡
    3. é€»è¾‘é¡ºåºè°ƒæ•´ - å…ˆé‡é‡‡æ ·å†å¤„ç†ï¼Œä¿è¯ Buffer æ•°æ®ä¸€è‡´æ€§
"""

import logging
import numpy as np
import sherpa_onnx
import soxr
import time
import wave
import datetime
import collections
import os
from pathlib import Path
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ASRResult:
    """ASR è¯†åˆ«ç»“æœ"""

    text: str
    is_final: bool


class LocalASR:
    """æœ¬åœ° ASR å°è£…ï¼Œä½¿ç”¨ Sherpa-onnx Paraformer æµå¼æ¨¡å‹"""

    TARGET_SAMPLE_RATE = 16000  # sherpa-onnx æ¨¡å‹è¦æ±‚ 16kHz

    # --- VAD é…ç½®è°ƒä¼˜ ---
    VAD_START_THRESHOLD = 0.025  # é—¨é™ï¼šç¨å¾®é™ä½ä»¥æ•æ‰å¼±éŸ³ (åŸ 0.05)
    VAD_END_THRESHOLD = 0.015
    SILENCE_FRAMES_FOR_ENDPOINT = 25  # çº¦ 1-1.5ç§’é™éŸ³æ–­å¥

    # é¢„è¯»ç¼“å†²å¤§å° (Lookback Buffer)
    # å‡è®¾æ¯æ¬¡å¤„ç† chunk çº¦ä¸º 20-40msï¼Œ25 ä¸ª chunk å¤§çº¦èƒ½å›æº¯ 0.5~1.0 ç§’
    # è¿™å†³å®šäº†"è¯´è¯å‰"èƒ½æ‰¾å›å¤šå°‘éŸ³é¢‘
    WINDOW_BUFFER_SIZE = 25

    def __init__(self, models_dir: Path | None = None, num_threads: int = 4):
        """
        åˆå§‹åŒ– ASR è¯†åˆ«å™¨
        """
        if models_dir is None:
            # å‡è®¾å½“å‰æ–‡ä»¶åœ¨ services/asr.pyï¼Œæ¨¡å‹åœ¨é¡¹ç›®æ ¹ç›®å½• models
            models_dir = Path(__file__).parent.parent / "models"

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (å…¼å®¹ int8 å’Œ fp32)
        encoder_path = models_dir / "encoder.int8.onnx"
        if not encoder_path.exists():
            encoder_path = models_dir / "encoder.onnx"
            decoder_path = models_dir / "decoder.onnx"
        else:
            decoder_path = models_dir / "decoder.int8.onnx"

        if not encoder_path.exists():
            raise FileNotFoundError(f"ASR æ¨¡å‹æœªæ‰¾åˆ°: {models_dir}")

        logger.info(f"åŠ è½½ ASR æ¨¡å‹: {models_dir}")

        # sherpa-onnx è¯†åˆ«å™¨
        self.recognizer = sherpa_onnx.OnlineRecognizer.from_paraformer(
            tokens=str(models_dir / "tokens.txt"),
            encoder=str(encoder_path),
            decoder=str(decoder_path),
            num_threads=num_threads,
            sample_rate=self.TARGET_SAMPLE_RATE,
            feature_dim=80,
            decoding_method="greedy_search",
            enable_endpoint_detection=False,  # ç¦ç”¨å†…ç½® VADï¼Œä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘
            debug=False,
        )

        self._stream = self.recognizer.create_stream()
        self._last_partial_text = ""

        # soxr é‡é‡‡æ ·å™¨ç¼“å­˜
        self._resamplers: dict[int, soxr.ResampleStream] = {}

        # VAD çŠ¶æ€
        self._is_speaking = False  # æ˜¯å¦æ­£åœ¨è¯´è¯
        self._silence_frame_count = 0  # è¿ç»­é™éŸ³å¸§è®¡æ•°
        self._max_rms = 0.0  # å½“å‰è¯­éŸ³æ®µçš„æœ€å¤§ RMS

        # --- æ–°å¢ï¼šç¯å½¢ç¼“å†² (Lookback Buffer) ---
        # ç”¨äºå­˜å‚¨"è§¦å‘è¯´è¯å‰"çš„ä¸€å°æ®µéŸ³é¢‘ï¼Œé˜²æ­¢å¼€å¤´è¢«åˆ‡æ‰
        self._window_buffer = collections.deque(maxlen=self.WINDOW_BUFFER_SIZE)

        # è°ƒè¯•å½•éŸ³é…ç½®
        self._audio_buffer = []  # ä»…ç”¨äºä¿å­˜æ–‡ä»¶è°ƒè¯•
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œé¿å…ç¡¬ç¼–ç 
        self.debug_dir = Path("debug_records")
        self.debug_dir.mkdir(parents=True, exist_ok=True)

        logger.info("ASR å¼•æ“åˆå§‹åŒ–å®Œæˆ (å¸¦ VAD é¢„è¯»ç¼“å†²)")

    def reset(self):
        """é‡ç½®è¯†åˆ«æµå’Œç¼“å†²åŒº"""
        self._stream = self.recognizer.create_stream()
        self._last_partial_text = ""
        self._is_speaking = False
        self._silence_frame_count = 0
        self._max_rms = 0.0
        self._audio_buffer = []
        # æ³¨æ„ï¼šresetæ—¶ä¸æ¸…é™¤ _window_bufferï¼Œä¿æŒç¯å¢ƒéŸ³çš„è¿ç»­æ€§

    def _save_debug_audio(self):
        """ä¿å­˜å½“å‰ç¼“å†²åŒºçš„éŸ³é¢‘åˆ°æ–‡ä»¶ (ç”¨äºæ’æŸ¥æ–­éŸ³é—®é¢˜)"""
        if not self._audio_buffer:
            return

        try:
            # æ‹¼æ¥éŸ³é¢‘æ•°æ®
            audio_data = np.concatenate(self._audio_buffer)
            # float32 [-1, 1] -> int16
            audio_int16 = (audio_data * 32767).clip(-32768, 32767).astype(np.int16)

            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            filename = self.debug_dir / f"asr_{timestamp}.wav"

            with wave.open(str(filename), "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.TARGET_SAMPLE_RATE)
                wf.writeframes(audio_int16.tobytes())

            logger.info(f"å·²ä¿å­˜è°ƒè¯•å½•éŸ³: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜è°ƒè¯•å½•éŸ³å¤±è´¥: {e}")

    @staticmethod
    def _calculate_rms(audio_data: np.ndarray) -> float:
        """è®¡ç®—éŸ³é¢‘çš„ RMS èƒ½é‡"""
        if len(audio_data) == 0:
            return 0.0
        return float(np.sqrt(np.mean(audio_data**2)))

    def _get_resampler(self, source_rate: int) -> soxr.ResampleStream | None:
        """è·å–é‡é‡‡æ ·å™¨"""
        if source_rate == self.TARGET_SAMPLE_RATE:
            return None

        if source_rate not in self._resamplers:
            self._resamplers[source_rate] = soxr.ResampleStream(
                source_rate,
                self.TARGET_SAMPLE_RATE,
                num_channels=1,
                dtype=np.float32,
            )
            logger.info(f"åˆ›å»ºé‡é‡‡æ ·å™¨: {source_rate}Hz -> {self.TARGET_SAMPLE_RATE}Hz")

        return self._resamplers[source_rate]

    def process_audio(
        self, audio_data: np.ndarray, sample_rate: int = 48000
    ) -> ASRResult | None:
        """
        æ ¸å¿ƒå¤„ç†é€»è¾‘ - åŒé˜ˆå€¼ VAD + é¢„è¯»ç¼“å†²
        """
        start_t = time.perf_counter()

        # 1. é‡é‡‡æ ·
        resampler = self._get_resampler(sample_rate)
        if resampler is not None:
            audio_data = resampler.resample_chunk(audio_data)

        # 2. è®¡ç®— RMS
        rms = self._calculate_rms(audio_data)

        # --- å…³é”®é€»è¾‘ï¼šåŒé˜ˆå€¼åˆ¤å®š ---
        if not self._is_speaking:
            # é™éŸ³çŠ¶æ€ï¼šå¿…é¡»å†²è¿‡è¾ƒé«˜çš„ START é˜ˆå€¼æ‰èƒ½æ¿€æ´»
            is_voice_active = rms >= self.VAD_START_THRESHOLD
        else:
            # è¯´è¯çŠ¶æ€ï¼šåªè¦ç»´æŒåœ¨è¾ƒä½çš„ END é˜ˆå€¼ä»¥ä¸Šå°±ç®—å»¶ç»­
            is_voice_active = rms >= self.VAD_END_THRESHOLD

        # 3. ç»´æŠ¤æ»‘åŠ¨çª—å£ (é™éŸ³æ—¶å­˜æ•°æ®ï¼Œæ–¹ä¾¿å›å¤´è¡¥æ•‘)
        if not self._is_speaking:
            self._window_buffer.append(audio_data)

            # (å¯é€‰) è°ƒè¯•æ‰“å°ï¼šå¸®ä½ é€šè¿‡æ—¥å¿—é€šè¿‡ RMS åˆ°åº•è¯¥è®¾å¤šå°‘
            # å¦‚æœè§‰å¾—æ—¥å¿—å¤ªåµï¼Œå¯ä»¥æ³¨é‡Šæ‰
            if rms > 0.01:
                logger.info(f"ç¯å¢ƒå™ªéŸ³ RMS: {rms:.4f}")

        # 4. VAD çŠ¶æ€åˆ‡æ¢
        if is_voice_active:
            # åˆšå¼€å§‹æ£€æµ‹åˆ°å£°éŸ³ (0 -> 1)
            if not self._is_speaking:
                logger.info(
                    f"ğŸ™ï¸ è§¦å‘è¯­éŸ³ (RMS: {rms:.4f}) > é˜ˆå€¼ {self.VAD_START_THRESHOLD} "
                    f"- å›æº¯è¡¥å…¨ {len(self._window_buffer)} å¸§"
                )

                # æŠŠç¼“å†²åŒºé‡Œçš„â€œå¼€å¤´â€è¡¥è¿›å» (æ•‘å›å¼±éŸ³çš„å…³é”®ï¼)
                for past_chunk in self._window_buffer:
                    self._audio_buffer.append(past_chunk)
                    self._stream.accept_waveform(self.TARGET_SAMPLE_RATE, past_chunk)

                self._window_buffer.clear()

            self._is_speaking = True
            self._silence_frame_count = 0
            if rms > self._max_rms:
                self._max_rms = rms

        # 5. é€å…¥è¯†åˆ«å™¨
        if self._is_speaking:
            self._audio_buffer.append(audio_data)
            self._stream.accept_waveform(self.TARGET_SAMPLE_RATE, audio_data)

            while self.recognizer.is_ready(self._stream):
                self.recognizer.decode_stream(self._stream)

        # 6. é™éŸ³æ–­å¥æ£€æµ‹ (1 -> 0)
        if not is_voice_active and self._is_speaking:
            self._silence_frame_count += 1

            # åªæœ‰è¿ç»­ N å¸§ä½äº END_THRESHOLD æ‰åˆ‡æ–­
            if self._silence_frame_count >= self.SILENCE_FRAMES_FOR_ENDPOINT:
                text = self.recognizer.get_result(self._stream).strip()

                logger.info(
                    f"ğŸ›‘ è¯´è¯ç»“æŸ (é™éŸ³è®¡æ•°: {self._silence_frame_count}) | "
                    f"å³°å€¼ RMS: {self._max_rms:.4f}"
                )

                self._save_debug_audio()
                self.reset()

                if text:
                    logger.info(f"ASR æœ€ç»ˆè¯†åˆ«: {text}")
                    return ASRResult(text=text, is_final=True)
                return None

        # 7. ä¸­é—´ç»“æœ
        if is_voice_active and self._is_speaking:
            text = self.recognizer.get_result(self._stream).strip()
            if text and text != self._last_partial_text:
                self._last_partial_text = text
                return ASRResult(text=text, is_final=False)

        return None

    @staticmethod
    def audio_frame_to_float32(frame_data: bytes, num_channels: int = 1) -> np.ndarray:
        """
        [ä¿®å¤ç‰ˆ] å®‰å…¨è½¬æ¢éŸ³é¢‘æ ¼å¼ int16 -> float32
        ä¿®å¤äº†åŒå£°é“å¹³å‡å¯¼è‡´éŸ³é‡å‡åŠçš„é—®é¢˜
        """
        # ç¡®ä¿å­—èŠ‚æµå¯¹é½
        if len(frame_data) % 2 != 0:
            frame_data = frame_data[:-1]

        audio_int16 = np.frombuffer(frame_data, dtype=np.int16)

        # å¤„ç†åŒå£°é“
        if num_channels == 2:
            try:
                audio_reshaped = audio_int16.reshape(-1, 2)

                # --- å…³é”®ä¿®å¤ ---
                # ä¹‹å‰ä½¿ç”¨ np.mean ä¼šå¯¼è‡´ (äººå£°+é™éŸ³)/2 = éŸ³é‡å‡åŠ
                # ç°åœ¨åªå–å·¦å£°é“ (é€šå¸¸ Channel 0 æ˜¯ User)
                audio_mono = audio_reshaped[:, 0]

                return audio_mono.astype(np.float32) / 32768.0
            except Exception as e:
                logger.warning(f"åŒå£°é“æå–å¤±è´¥: {e}ï¼Œå›é€€åˆ°åŸå§‹æ··åˆ")
                pass

        # å•å£°é“æˆ–å›é€€æƒ…å†µï¼šå½’ä¸€åŒ–åˆ° [-1, 1]
        return audio_int16.astype(np.float32) / 32768.0
